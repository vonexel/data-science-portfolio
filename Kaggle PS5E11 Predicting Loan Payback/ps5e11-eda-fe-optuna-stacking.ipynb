{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"height: auto; position: relative; background: #f0f0f0; margin-bottom: 20px; padding: 30px 0;\">\n    <div style=\"display: inline-block; background: #003247; color: #fff; font-size: 26px; font-weight: 700; padding: 30px;\">\n        PS5E11: Predicting Loan Payback | EDA üìä | FE ‚öôÔ∏è|  CatBoost üöÄ | Optuna üî¨ \n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"description-goal\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Description & Goal üì£</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# **Overview** \n\n    \nThis task involves predicting the probability that a borrower will pay back their loan based on a comprehensive features encompassing socioeconomic demographics, financial indicators, and institutional risk classifications. The problem represents a canonical binary classification challenge within credit risk analytics, where accurate prediction directly impacts financial institution's capital allocation efficiency and systemic stability. The primary challenge in credit risk assessment emerges from the absence of established credit histories, which fundamentally impedes the determination of conditional risk categorization and the assignment of meaningful credit scores. In such a case, one should rely on a high-quality and well-justified feature engineering section and meticulously select the derived features for subsequent model selection and training for predictions.\n\n\n\nSubmissions are scored on [area under the ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target. The **AUC ROC** quantifies a model‚Äôs ability to distinguish between positive and negative classes by measuring the probability that a randomly selected positive instance is ranked higher than a randomly selected negative instance. It is defined as:","metadata":{}},{"cell_type":"markdown","source":"\\\\[\n\\text{AUC} = \\frac{|{(i,j) \\in S : y_i = 1, y_j = 0 \\text{ and } \\hat{y}_i > \\hat{y}_j}|}{|S|}\n\\\\]\n\n\n<div style=\"display: flex; align-items: flex-start;\">\n  <div style=\"flex: 1; max-width: 35%; padding-right: 20px;\">\n    <img src=\"https://s3.amazonaws.com/media-p.slid.es/uploads/1094055/images/10690666/pasted-from-clipboard.png\" \n         style=\"max-height: 400px; max-width: 100%; object-fit: contain;\">\n  </div>\n  <div style=\"flex: 2;\">\n    <p>where:</p>\n    <ul>\n      <li>$y_i, y_j$ ‚Äî true binary labels (1 = positive class, 0 = negative class),</li>\n      <li>$\\hat{y}_i, \\hat{y}_j$ ‚Äî predicted probabilities for the positive class,</li>\n      <li>$S$ ‚Äî set of all pairs with $y_i \\neq y_j$,</li>\n      <li>$|S|$ ‚Äî total number of such pairs.</li>\n    </ul>\n    <p>The <b>AUC</b> reflects the proportion of correctly ordered pairs: for every pair of instances where the true label of $i$ is positive and $j$ is negative, the model's predicted probability for $i$ should exceed that for $j$.</p>\n    <p>The metric score ranges from <b>`0`</b> to <b>`1`</b>:</p>\n    <ul>\n      <li><b>`0.5`</b> ‚Äî the model performs no better than random guessing (equivalent to a diagonal ROC curve);</li>\n      <li><b>`1.0`</b> ‚Äî the model perfectly separates the classes (all positive instances are ranked above negative ones);</li>\n      <li><b>`&lt; 0.5`</b> ‚Äî the model's predictions are inversely correlated with the true labels (worse than random; flipping predictions would improve performance).</li>\n    </ul>\n    <p><b>AUC</b> is <i>threshold-agnostic</i>, meaning it evaluates model performance across all possible classification thresholds, making it ideal for imbalanced datasets where accuracy or precision may be misleading.</p>\n  </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"table-of-contents\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Table of Contents üî†</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"* [1. Description & Goal üì£](#description-goal)\n* [2. Table of Contents üî†](#table-of-contents)\n* [3. Import üìö](#import)\n* [4. $\\lambda$uxiliary Functions ‚öóÔ∏è](#auxiliary-func)\n* [5. Load Data üíæ](#data-loading)\n* [5.1 Data Understanding üóÉÔ∏èüßê](#data-understanding)\n* [5.2 HeatMap üî•üó∫Ô∏è + Clustering üß∂ = Cluster Map üèúÔ∏è](#cluster-map)\n* [6. EDA üïµ | üìä](#eda)\n* [7. Feature Engineering üõ†Ô∏è | ‚öôÔ∏è](#fe)\n* [8. Modeling ü§ñ](#modeling)\n* [8.1 Optuna üî¨](#optuna)\n* [8.2 CatBoost üêà | üöÄ](#catboost)\n* [8.3 XgBoost ü§ñ](#xgb)\n* [8.4 LightGBM üëæ](#lgb)\n* [8.5 Stacking üß±|üß†](#stack)\n* [9. Submission ‚úîÔ∏è](#submission)\n* [10. Summary üí≠](#summary)\n* [11. References üìú](#refs)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"import\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Import üìö</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport joblib\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom matplotlib.colors import Normalize\nfrom matplotlib.patches import Rectangle\nfrom scipy.stats import gaussian_kde, probplot\nfrom statsmodels.graphics.mosaicplot import mosaic\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu\n\n\nimport shap\nimport optuna\nimport joblib\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.pipeline import Pipeline\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, PowerTransformer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n\n\n\npd.reset_option('display.max_columns')\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:03.929232Z","iopub.execute_input":"2025-11-29T07:20:03.929490Z","iopub.status.idle":"2025-11-29T07:20:17.919957Z","shell.execute_reply.started":"2025-11-29T07:20:03.929468Z","shell.execute_reply":"2025-11-29T07:20:17.919168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"auxiliary-func\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>$\\lambda$uxiliary Functions ‚öóÔ∏è</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Functions for preprocesssing input, losses, visualization, setting seed for consistent results and etc.","metadata":{}},{"cell_type":"code","source":"# setting seed\nSEED = 64911002\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nseed_everything(SEED)\n\n\ndef nan_check(df):\n    total_entries = df.shape[0] * df.shape[1]\n    missing_entries_max = df.isnull().sum().sum()\n    missing_entries_max_percentage = (missing_entries_max / total_entries) * 100\n    print(f\"Total entries in the dataset: {total_entries}\")\n    print(f\"Maximum missing values in the dataset: {missing_entries_max}\")\n    print(f\"Percentage of maximum missing values in the dataset: {missing_entries_max_percentage:.2f}%\")\n    return df.isna().sum()\n\n\ndef feature_value_counts(df, columns_to_include=None):\n    \"\"\"\n    Outputs the Distribution of values for each attribute in the DataFrame.\n    \n    Parameters:\n    df (pd.DataFrame): Input DataFrame\n    columns_to_include (list, optional): List of column names to process. \n                                        If None, all columns will be processed.\n    \"\"\"\n    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n    columns = columns_to_include if columns_to_include is not None else df.columns\n    \n    for column in columns:\n        if column not in df.columns:\n            print(f\"Warning: Column '{column}' not found in DataFrame. Skipping.\")\n            continue\n            \n        print(f\"Distribution of '{column}' feature:\")\n        print(df[column].value_counts())\n        print(\"\\n\")\n\n\ndef feature_distribution(df, columns_to_visualize=None, exclude_from_visualization=None):\n    \"\"\"\n    Visualizes feature distributions with division into numerical and categorical ones\n    \"\"\"\n    if columns_to_visualize is None:\n        columns_to_visualize = df.columns.tolist()\n    if exclude_from_visualization:\n        columns_to_visualize = [col for col in columns_to_visualize if col not in exclude_from_visualization]\n    num_cols = df[columns_to_visualize].select_dtypes(include=np.number).columns.tolist()\n    cat_cols = df[columns_to_visualize].select_dtypes(exclude=np.number).columns.tolist()\n    \n    # numerical\n    if num_cols:\n        fig_num, axes_num = plt.subplots(\n            len(num_cols), 3, \n            figsize=(24, 6 * len(num_cols)),\n            squeeze=False\n        )\n        \n        for i, col in enumerate(num_cols):\n            ax_hist = axes_num[i, 0]\n            ax_box = axes_num[i, 1]\n            ax_stat = axes_num[i, 2]\n            sns.histplot(df[col], kde=True, ax=ax_hist, bins=30, color='steelblue')\n            mean_val = df[col].mean()\n            std_val = df[col].std()\n            ax_hist.axvline(mean_val, color='darkorange', linestyle='--', label=f'Mean: {mean_val:.2f}')\n            ax_hist.axvline(mean_val - std_val, color='red', linestyle=':', label='¬±1 Std')\n            ax_hist.axvline(mean_val + std_val, color='red', linestyle=':')\n            ax_hist.legend()\n            \n            # box-plot with outliers detection\n            sns.boxplot(y=df[col], ax=ax_box, color='steelblue')\n            q1, q3 = df[col].quantile([0.25, 0.75])\n            iqr = q3 - q1\n            lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n            \n            if df[col].min() < lower:\n                ax_box.add_patch(Rectangle(\n                    (-0.4, df[col].min()), 0.8, lower - df[col].min(),\n                    edgecolor='red', linestyle='--', fill=False))\n            if df[col].max() > upper:\n                ax_box.add_patch(Rectangle(\n                    (-0.4, upper), 0.8, df[col].max() - upper,\n                    edgecolor='red', linestyle='--', fill=False))\n            \n            # stats\n            stats = [\n                f\"Mean: {mean_val:.2f}\",\n                f\"Std: {std_val:.2f}\",\n                f\"Min: {df[col].min():.2f}\",\n                f\"Max: {df[col].max():.2f}\",\n                f\"Skew: {df[col].skew():.2f}\",\n                f\"Kurtosis: {df[col].kurtosis():.2f}\"\n            ]\n            ax_stat.axis('off')\n            ax_stat.text(0.1, 0.5, '\\n'.join(stats), fontsize=12, va='center')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    # categorical\n    if cat_cols:\n        fig_cat, axes_cat = plt.subplots(\n            len(cat_cols), 2, \n            figsize=(20, 5 * len(cat_cols)),\n            squeeze=False\n        )\n        \n        for i, col in enumerate(cat_cols):\n            ax_count = axes_cat[i, 0]\n            ax_top = axes_cat[i, 1]\n            \n            # plot n-categories\n            top_categories = df[col].value_counts().nlargest(20).index\n            df_plot = df[col].apply(lambda x: x if x in top_categories else 'Other')\n            sns.countplot(x=df_plot, ax=ax_count, palette='coolwarm')\n            ax_count.tick_params(axis='x', rotation=90)\n            \n            # plot 5-categories\n            top5 = df[col].value_counts().nlargest(5)            \n            sns.barplot(x=top5.values, y=top5.index, ax=ax_top, palette='coolwarm', orient='h')\n        plt.tight_layout()\n        plt.show();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:17.921101Z","iopub.execute_input":"2025-11-29T07:20:17.921569Z","iopub.status.idle":"2025-11-29T07:20:17.936268Z","shell.execute_reply.started":"2025-11-29T07:20:17.921553Z","shell.execute_reply":"2025-11-29T07:20:17.935456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"data-loading\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Load Data üíæ</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/playground-series-s5e11/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e11/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:17.937087Z","iopub.execute_input":"2025-11-29T07:20:17.937429Z","iopub.status.idle":"2025-11-29T07:20:19.366231Z","shell.execute_reply.started":"2025-11-29T07:20:17.937400Z","shell.execute_reply":"2025-11-29T07:20:19.365634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:19.367661Z","iopub.execute_input":"2025-11-29T07:20:19.367930Z","iopub.status.idle":"2025-11-29T07:20:19.394803Z","shell.execute_reply.started":"2025-11-29T07:20:19.367912Z","shell.execute_reply":"2025-11-29T07:20:19.394225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:19.395452Z","iopub.execute_input":"2025-11-29T07:20:19.395691Z","iopub.status.idle":"2025-11-29T07:20:19.407030Z","shell.execute_reply.started":"2025-11-29T07:20:19.395674Z","shell.execute_reply":"2025-11-29T07:20:19.406291Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Based on  [source](https://www.kaggle.com/datasets/nabihazahid/loan-prediction-dataset-2025/data):\n\n\n1. `id` - an unique identifier for each record;\n2. `annual_income` - borrower's yearly income;\n3. `debt_to_income_ratio` - ratio of borrower‚Äôs debt to their income. Lower = better;\n4. `credit_score` - credit bureau score (e.g., FICO). Higher = less risky;\n5. `loan_amount` - amount of loan taken;\n6. `interest_rate` - loan par annual interest rate (%);\n7. `gender` - borrower's gender (Male/Female);\n8. `marital_status` - marital status (Single, Married, Divorced);\n9. `education_level` -  education level (High School, Bachelor, Master, PhD);\n10. `employment_status` - current employment type (Employed, Self-Employed, Unemployed);\n11. `loan_purpose` - loan purpose (Car, Education, Home, Medical, etc.);\n12. `grade_subgrade` - risk category assigned to loan (A1, B2, etc.);\n13. `loan_paid_back` - target variable (`0` = borrower defaulted (did not repay fully), `1` = borrower paid loan in full)\n\n\nEach row represents an individual loan applicant's financial and demographic information, with features describing creditworthiness indicators and a binary target variable **`loan_paid_back`** indicating whether the borrower fully repaid their loan **`1`** or defaulted **`0`**.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h2 id = \"data-understanding\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Data Understanding üóÉÔ∏èüßê</b>\n    </h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Check the data types","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:19.407902Z","iopub.execute_input":"2025-11-29T07:20:19.408142Z","iopub.status.idle":"2025-11-29T07:20:19.590639Z","shell.execute_reply.started":"2025-11-29T07:20:19.408105Z","shell.execute_reply":"2025-11-29T07:20:19.590011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:19.591325Z","iopub.execute_input":"2025-11-29T07:20:19.591569Z","iopub.status.idle":"2025-11-29T07:20:19.669137Z","shell.execute_reply.started":"2025-11-29T07:20:19.591540Z","shell.execute_reply":"2025-11-29T07:20:19.668355Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset contains both categorical and numerical features","metadata":{}},{"cell_type":"markdown","source":"## Check the shape of data","metadata":{}},{"cell_type":"code","source":"print(f\"Train dataset has {df.shape[0]} rows and {df.shape[1]} columns\")\nprint(f\"Test dataset has {test.shape[0]} rows and {test.shape[1]} columns\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:19.670048Z","iopub.execute_input":"2025-11-29T07:20:19.670295Z","iopub.status.idle":"2025-11-29T07:20:19.675030Z","shell.execute_reply.started":"2025-11-29T07:20:19.670278Z","shell.execute_reply":"2025-11-29T07:20:19.674261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check the missing values","metadata":{}},{"cell_type":"code","source":" nan_check(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:19.675810Z","iopub.execute_input":"2025-11-29T07:20:19.676122Z","iopub.status.idle":"2025-11-29T07:20:19.999833Z","shell.execute_reply.started":"2025-11-29T07:20:19.676105Z","shell.execute_reply":"2025-11-29T07:20:19.999267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" nan_check(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:20.002225Z","iopub.execute_input":"2025-11-29T07:20:20.002422Z","iopub.status.idle":"2025-11-29T07:20:20.142484Z","shell.execute_reply.started":"2025-11-29T07:20:20.002409Z","shell.execute_reply":"2025-11-29T07:20:20.141751Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"None NaN values are seen in both datasets","metadata":{}},{"cell_type":"markdown","source":"## Check the unique values","metadata":{}},{"cell_type":"code","source":"unique = df.select_dtypes(exclude=['object']).nunique()\nunique = unique[unique == 1]\nprint(f\"Columns with only one unique value: {unique}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:20.143431Z","iopub.execute_input":"2025-11-29T07:20:20.143751Z","iopub.status.idle":"2025-11-29T07:20:20.229232Z","shell.execute_reply.started":"2025-11-29T07:20:20.143726Z","shell.execute_reply":"2025-11-29T07:20:20.228571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"None features with only one unique value","metadata":{}},{"cell_type":"code","source":"df.describe(include = 'all')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:20.229911Z","iopub.execute_input":"2025-11-29T07:20:20.230183Z","iopub.status.idle":"2025-11-29T07:20:20.744993Z","shell.execute_reply.started":"2025-11-29T07:20:20.230163Z","shell.execute_reply":"2025-11-29T07:20:20.744261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.describe(include = 'all')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:20.745804Z","iopub.execute_input":"2025-11-29T07:20:20.746076Z","iopub.status.idle":"2025-11-29T07:20:21.013992Z","shell.execute_reply.started":"2025-11-29T07:20:20.746051Z","shell.execute_reply":"2025-11-29T07:20:21.013342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The comparison between training and test sets reveals critical distributional shifts that warrant attention for model robustness:\n\n1. Interest Rate Drift: Evolving Risk Appetite\nA notable increase in the maximum interest rate is observed in the test set (`21.29%` VS `20.99%` in training);\n\n2. Annual Income Extrapolation Risk\nThe test set shows a `3.34%` lower maximum annual income (`380654`) compared to the training set (`393381`). This indicates the absence of ultra-high-income clients in the test data. Consequently, the model has not been validated on this segment and may demonstrate instability when making predictions for new, affluent clients, as it must extrapolate beyond the income range it was assessed on;\n\n3. Debt-to-Income Ratio Stability\nIn contrast to the other features, the Debt-to-Income Ratio shows remarkable stability. All key statistics are identical up to the fourth decimal place between the train and test sets. This indicates a high-quality, stratified split for this critical risk factor, ensuring that the model's performance on DTI is likely to be reliable and consistent.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h2 id = \"cluster-map\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>HeatMap üî•üó∫Ô∏è + Clustering üß∂ = Cluster Map üèúÔ∏è</b>\n    </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"numeric_df = df.select_dtypes(include=['number'])\ncorr_matrix = numeric_df.corr()\nsns.clustermap(corr_matrix, annot = True, fmt = '.2f', linewidths = .5, cmap='coolwarm', figsize = (16, 8))\nplt.show();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:21.014785Z","iopub.execute_input":"2025-11-29T07:20:21.015400Z","iopub.status.idle":"2025-11-29T07:20:21.783672Z","shell.execute_reply.started":"2025-11-29T07:20:21.015372Z","shell.execute_reply":"2025-11-29T07:20:21.782973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The analysis of numeric features revealed **five** distinct clusters:\n\n1. Credit-Interest Cluster (`credit_score`, `interest_rate`) exhibits a strong inverse relationship (`‚àí0.54`), indicating that lenders systematically adjust interest rates as a function of creditworthiness. Higher `credit_score` values correlate with lower `interest_rate`, reflecting risk-based pricing mechanisms.\n2. Debt-Repayment Cluster (`debt_to_income_ratio`, `loan_paid_back`) has a moderate negative correlation (`‚àí0.34`). This suggests that borrowers with higher `debt-to-income` ratios are less likely to fully repay loans, highlighting a potential causal link between financial burden and repayment behavior. This makes perfect business sense: as a client's debt burden increases relative to their income, their ability to repay a new loan decreases significantly.\n3. Isolated Features (`annual_income`, `loan_amount`) operate independently, showing negligible correlations (`< 0.06`) with all other variables. These features lack strong structural ties to the identified clusters, implying they capture distinct or unobserved dimensions of borrower risk.\n4. Credit-Quality Cluster (`credit_score`, `loan_paid_back`) is the strongest positive correlation (`0.23`). Clients with higher `credit_scores` are statistically more likely to repay their loans, which aligns with global lending practices.\n5. Risk-Based Pricing Cluster (`interest_rate`, `loan_paid_back`) (`-0.13`) suggests that interest rate alone is a less powerful predictor. Its effect is likely indirect and mediated through the credit score.\n\n\n\n\nThe dataset provides two very strong, logical signals for predicting loan repayment (`debt_to_income_ratio` and `credit_score`). Although `annual_income` and `loan_amount` show no direct linear relationship, they should not be discarded. It's plausible that a large loan is risky for a low-income borrower, even if the two features individually are not predictive. The low correlations between most features simplify the modeling process by avoiding multicollinearity issues. The lack of direct correlation from other financial features is not a drawback but rather a clear directive to explore more sophisticated feature engineering, particularly through **interaction terms** and **non-linear transformations**, to unlock their predictive potential.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"eda\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>EDA üïµ | üìä</b>\n    </h1>\n</div>\n\n","metadata":{}},{"cell_type":"markdown","source":"## Distribution of the target ","metadata":{}},{"cell_type":"code","source":"loan_counts = df['loan_paid_back'].value_counts()\n\nlabels = ['Paid (1)', 'Defaulted (0)']\nsizes = loan_counts.values\npercentages = [f'{(count/sum(sizes))*100:.1f}%' for count in sizes]\nplt.figure(figsize=(16, 8))\ncolors = sns.color_palette('coolwarm', n_colors=2)\ncolors = colors[::-1]\n\n# pie chart\nwedges, texts, autotexts = plt.pie(sizes, \n                                  labels=labels, \n                                  autopct='%1.1f%%',\n                                  colors=colors,\n                                  startangle=90,\n                                  explode=(0.05, 0.05),  \n                                  textprops={'fontsize': 14, 'fontweight': 'bold'},\n                                  wedgeprops={'edgecolor': 'black', 'linewidth': 1.5})\n\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontweight('bold')\n    autotext.set_fontsize(16)\n    \nplt.title('Loan Repayment Status Distribution', \n          fontsize=24, \n          fontweight='bold', \n          pad=20,\n          color='#2c3e50')\n\nlegend_labels = [\n    f'Paid Loans: {sizes[0]:,} ({percentages[0]})',\n    f'Defaulted Loans: {sizes[1]:,} ({percentages[1]})'\n]\n\nplt.legend(wedges, legend_labels,\n          title=\"Loan Status Details\",\n          loc=\"center left\",\n          bbox_to_anchor=(1, 0, 0.5, 1),\n          fontsize=14,\n          title_fontsize=16,\n          frameon=True,\n          framealpha=0.9,\n          edgecolor='#2c3e50')\n\ntotal_loans = sum(sizes)\nplt.text(0, 0, f'Total Loans\\n{total_loans:,}', \n         ha='center', \n         va='center', \n         fontsize=18, \n         fontweight='bold',\n         color='#2c3e50',\n         bbox=dict(facecolor='white', alpha=0.8, edgecolor='#2c3e50', boxstyle='round,pad=1'))\n\nplt.figtext(0.5, 0.02, \n           f'Insight: With a {percentages[0]} repayment rate, the data shows healthy performance\\n'\n           f'but the {percentages[1]} default rate represents significant recoverable risk exposure',\n           ha='center', \n           fontsize=14, \n           fontstyle='italic',\n           bbox=dict(facecolor='lightyellow', alpha=0.9, edgecolor='orange', boxstyle='round,pad=0.5'))\nplt.tight_layout(rect=[0, 0.05, 0.85, 1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:21.784441Z","iopub.execute_input":"2025-11-29T07:20:21.784710Z","iopub.status.idle":"2025-11-29T07:20:22.033392Z","shell.execute_reply.started":"2025-11-29T07:20:21.784693Z","shell.execute_reply":"2025-11-29T07:20:22.032749Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The pie chart presents a significant dataset of `593994` loan records, revealing a class distribution where `79.9%` (`474494` loans) were fully repaid while `20.1%` (`119500` loans) defaulted. This `4:1` **class imbalance** has profound implications for model development and evaluation.\n\nThe `20.1%` default rate within this substantial dataset presents both challenges and opportunities for predictive modeling. While the class imbalance complicates traditional modeling approaches, it underscores the critical importance of proper evaluation metrics and business-aligned model development. Rather than optimizing for overall accuracy, practitioners should focus on identifying the most at-risk borrowers, where even modest improvements in identifying default cases can yield substantial financial benefits. Future work must prioritize techniques that address class imbalance through cost-sensitive learning, threshold optimization, and appropriate evaluation metrics. The high default rate suggests this dataset could provide valuable insights into risk factors that standard lending models may overlook, potentially leading to more sophisticated and profitable risk assessment frameworks that balance portfolio growth with risk management.","metadata":{}},{"cell_type":"markdown","source":"## Distribution of numeric features","metadata":{}},{"cell_type":"code","source":"numeric_features = ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\nfeature_value_counts(df, columns_to_include = numeric_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:22.034151Z","iopub.execute_input":"2025-11-29T07:20:22.034770Z","iopub.status.idle":"2025-11-29T07:20:22.110329Z","shell.execute_reply.started":"2025-11-29T07:20:22.034739Z","shell.execute_reply":"2025-11-29T07:20:22.109722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_distribution(df, columns_to_visualize = numeric_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:22.111033Z","iopub.execute_input":"2025-11-29T07:20:22.111287Z","iopub.status.idle":"2025-11-29T07:20:35.815133Z","shell.execute_reply.started":"2025-11-29T07:20:22.111261Z","shell.execute_reply":"2025-11-29T07:20:35.814492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations**:\n\n* **'annual_income'**:\n\nThe distribution of values is characterized by a pronounced right-skew, as clearly evidenced by multiple analytical methods. The histogram reveals the core of the data is concentrated in the `25000 - 50000` range, with a sharp decline in frequency beyond `100000`. This visual assessment is quantitatively confirmed by the summary statistics: the mean of `48212.20` is substantially higher than the median of `46557.68`, a classic indicator of right-skewness, and the high skewness value of `1.72` confirms a substantial positive skew. Therefore, it is recommended to apply **logarithmic transformations** or **Winsorization at the 99th percentile** to mitigate the impact of these outliers. Finally, the significant deviation from normality in both skewness and kurtosis necessitates the use of non-parametric methods or appropriate data transformations before any parametric modeling is undertaken, a deviation that would be formally confirmed by a test like **Shapiro-Wilk**.\n\n\n* **'debt_to_income_ratio'**:\n\nThe DTI distribution displays pronounced positive skewness (`1.41`), with most clients clustered in a conservative range of `0.0 - 0.2`, peaking at `0.05 - 0.15`. While outliers exist above `0.3`, their low prevalence (`<1%`) suggests high DTI is uncommon. A critical hypothesis is the presence of two distinct client segments: a low-risk group (`DTI < 0.1`) and a moderate-risk group (`DTI 0.1 - 0.2`), which may reflect different underwriting standards. To enhance interpretability, binning DTI into risk tiers (`<0.1`, `0.1 - 0.2`, `>0.2`) may be considered.\n\n* **'credit_score'**:\n\nIn contrast to DTI, the Credit Score distribution is remarkably stable and approximates a normal distribution, with minimal skew (`-0.17`) and kurtosis (`0.10`). Its peak in the `650 - 700` range and symmetrical spread make it ideal for parametric modeling without transformation. However, the minimum value of 395 identifies a segment of clients with \"very poor\" credit, warranting specialized risk assessment. It is recommended to investigate non-linear relationships with the target variable using spline regression.\n\n\n* **'loan_amount'**:\n\nThe Loan Amount feature exhibits mild right skewness (`0.21`) and high variability, with most loans between `5000 - 25000`. The presence of dual peaks near `10000-15000` suggests these may correspond to standardized loan products (e.g., `debt consolidation` VS `education loans`). Outliers above `35000`, while fewer than in other features, could indicate elevated default risk, especially when combined with low income. Therefore, it is critical to analyze interaction terms between `loan_amount`, `loan_purpose`  `annual_income`.\n\n* **'interest_rate'**:\n\nThe Interest Rate distribution demonstrates near-perfect symmetry (skewness = `0.05`) and low variability (`2.01%`), reflecting a disciplined and consistent pricing strategy.","metadata":{}},{"cell_type":"markdown","source":"## Distribution of categorical features","metadata":{}},{"cell_type":"code","source":"categorical_features = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\nfeature_value_counts(df, columns_to_include = categorical_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:35.815980Z","iopub.execute_input":"2025-11-29T07:20:35.816310Z","iopub.status.idle":"2025-11-29T07:20:36.061646Z","shell.execute_reply.started":"2025-11-29T07:20:35.816290Z","shell.execute_reply":"2025-11-29T07:20:36.060862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_distribution(df, columns_to_visualize = categorical_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:36.062454Z","iopub.execute_input":"2025-11-29T07:20:36.062668Z","iopub.status.idle":"2025-11-29T07:20:41.310736Z","shell.execute_reply.started":"2025-11-29T07:20:36.062643Z","shell.execute_reply":"2025-11-29T07:20:41.310003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations**:\n\n* **'gender'**:\n\nThe bar chart reveals a significant gender imbalance, with `Female` (`306175`) as the dominant category, `Male` (`284091`) as the secondary category, and `Other` (`3728`) representing a negligible fraction of the dataset. The slight female majority (51.5% vs. 48.3%) suggests either demographic trends in loan applications or potential sampling bias. It indicates gender-neutral application patterns, but potential differences in default rates by gender warrant investigation. Maybe consider gender as a potential interaction term with other risk factors (e.g., **income-to-debt ratio**) rather than a primary predictor.\n\n\n* **'marital_status'**:\n\nThe distribution shows four distinct categories with a clear dominance of `Single` (`288843`) applicants, followed by `Married` (`277239`), while `Divorced` (`21312`) and `Widowed` (`6600`) represent small minority segments. The combined `Single` and `Married` categories represent over 95% of the dataset, suggesting these are the primary life stages of credit applicants. `Widowed` and `Divorced` groups have insufficient data for reliable risk modeling. This could lead to poor model performance for these demographics.\n\n\n* **'education_level'**:\n\nThe bar chart shows a clear hierarchy of education levels, with `Bachelor's` as the most common (`279606`), followed by `High School` (`183592`), `Master's` (`93097`), and minimal representation of `PhD` (`11022`) and `Other` (`26677`). The strong representation of Bachelor's degree holders suggests targeting this demographic in marketing efforts. Perhaps we should investigate the `Other` category to determine if it can be meaningfully subdivided and analyze default rates by education level to identify potential risk patterns.\n\n* **'employment_status'**:\n\nThe chart reveals extreme imbalance, with `Employed` overwhelmingly dominating (`450645`), while all other categories represent small minority segments (`Unemployed` (`62485`), `Self-employed` (`52480`), `Retired` (`16453`), `Student` (`11931`)). The high percentage of employed applicants (`75.9%`) suggests the dataset primarily captures individuals with stable income sources. It's necessary to investigate whether unemployment is correlated with higher default rates, potentially indicating economic vulnerability. We should consider the possibility of creation of a binary `Employed VS Non-Employed` feature for modeling stability or combining `Retired` and `Student` into an `Other` category due to low sample sizes.\n\n* **'loan_purpose'**:\n\nThe chart shows extreme concentration on `Debt consolidation` (`324695`), with all other categories representing significantly smaller proportions (`Other` (`63874`), `Car` (`58108`), `Home` (`44118`), `Education` (`36641`), `Business` (`35303`), `Medical` (`22806`), `Vacation` (`8449`)). Different loan purposes may carry different risk profiles (e.g., education loans might have higher repayment likelihood than vacation loans). The overwhelming focus on `debt consolidation` (`54.7%`) suggests a specific market segment with potentially unique risk characteristics. It's necessary to analyze the aspects related to the composition of the `Other` category to determine if it can be meaningfully subdivided or create a binary feature for `debt consolidation VS other purposes` as a strong potential predictor.\n\n* **'grade_subgrade'**:\n\nThe chart displays a hierarchical structure with `30 unique subgrades`, showing clear concentration in the `C-tier` (particularly `C3` (`58695`) and `C4` (`55957`)), with declining frequency as credit quality improves (`B`, `A tiers`) or worsens (`D`, `E`, `F`, `G` tiers). The `C-tier` subgrades (`C1-C5`) represent approximately `42%` of all loans, indicating this is the core market segment. The distribution follows an expected pattern with higher frequency in middle-risk categories (`C`, `B`) and lower frequency in both high-quality (`A`) and high-risk (`D-G`) segments. \n\n\n\nThe categorical features provide valuable segmentation opportunities but require strategic engineering to address **class imbalance**. The most valuable features for risk modeling will be `grade_subgrade` (for direct risk assessment) and `loan_purpose` (for behavioral insight), while other features may serve better as interaction terms rather than primary predictors. **Special attention** needed for minority segments to prevent model bias.","metadata":{}},{"cell_type":"markdown","source":"## Numeric Feature Distributions by Loan Repayment Status","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\nfor i, feature in enumerate(numeric_features):\n    sns.boxplot(x='loan_paid_back', y=feature, data=df, ax=axes[i], palette='coolwarm')\n    axes[i].set_title(f'Distribution of {feature} by target', fontweight='bold')\n    group_0 = df[df['loan_paid_back'] == 0][feature]\n    group_1 = df[df['loan_paid_back'] == 1][feature]\n    t_stat, p_value = stats.ttest_ind(group_0, group_1, nan_policy='omit')\n    axes[i].text(0.5, 0.95, f'p-value: {p_value:.4f}', transform=axes[i].transAxes,\n                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\nfig.delaxes(axes[5])\nplt.tight_layout()\nplt.show();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:41.311700Z","iopub.execute_input":"2025-11-29T07:20:41.311928Z","iopub.status.idle":"2025-11-29T07:20:42.695993Z","shell.execute_reply.started":"2025-11-29T07:20:41.311906Z","shell.execute_reply":"2025-11-29T07:20:42.695315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observation**:\n\nBoth groups (repaid and defaulted) exhibit similarly right-skewed distributions, with the majority of borrowers concentrated in the lower income ranges (e.g., below `100000`) and a long tail of high-income borrowers. The two distributions likely overlap significantly, indicating that income alone is not a strong discriminator between those who repay and those who default. This aligns with the near-zero correlation coefficient (`0.00`) observed in the correlation matrix.The highly significant p-values across most features validate the underlying risk assessment framework. \n\nStatistically significant disparities exist across all credit risk features when stratified by repayment status, with debt-to-income ratio (DTI) emerging as the strongest predictor (p = `0`) showing non-overlapping IQRs between groups. Credit score and annual income demonstrate robust predictive power (both p = `0`), with 50-point score gaps and meaningful income thresholds separating repayment behaviors, while loan amount shows only marginal significance (p = `0.0037`) with overlapping distributions. Interest rates reveal a critical duality ‚Äì higher rates for defaulters (p = `0`) validate risk-based pricing yet introduce causality concerns where elevated rates themselves may increase default probability. These findings necessitate DTI-focused underwriting thresholds, non-linear feature engineering for income and credit scores, and causal analysis to optimize risk-based pricing without inadvertently amplifying defaults through rate structures.","metadata":{}},{"cell_type":"markdown","source":"## Categorical Feature Distributions by Loan Repayment Status","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 3, figsize=(20, 12))\naxes = axes.ravel()\n\n\nfor i, feature in enumerate(categorical_features):\n    category_performance = df.groupby(feature)['loan_paid_back'].mean().sort_values(ascending=False)\n    if feature == 'grade_subgrade':\n        bars = axes[i].barh(category_performance.index, category_performance.values, \n                           color=plt.cm.coolwarm(category_performance.values))\n        axes[i].set_title(f'Loan Repayment Percentage by {feature}', fontweight='bold', fontsize=16)\n        axes[i].set_xlabel('Repayment percentage', fontsize=14)\n        axes[i].set_ylabel(feature, fontsize=14)\n        for j, bar in enumerate(bars):\n            width = bar.get_width()\n            x_pos = width + 0.01 if width < 0.9 else width - 0.03\n            ha = 'left' if width < 0.9 else 'right'\n            axes[i].annotate(f'{width:.3f}', \n                            (x_pos, bar.get_y() + bar.get_height()/2),\n                            ha=ha, va='center', fontweight='bold', fontsize=10)\n        axes[i].set_xlim(0, max(category_performance.values) * 1.15)\n        \n    else:\n        barplot = sns.barplot(x=category_performance.index, y=category_performance.values, \n                            ax=axes[i], palette='coolwarm')\n        axes[i].set_title(f'Loan Repayment Percentage by {feature}', fontweight='bold', fontsize=16)\n        axes[i].set_ylabel('Repayment percentage', fontsize=14)\n        axes[i].set_xlabel('')\n        axes[i].tick_params(axis='x', rotation=25, labelsize=12)\n        for p in barplot.patches:\n            height = p.get_height()\n            y_pos = height + 0.01 if height < 0.95 else height - 0.04\n            va = 'bottom' if height < 0.95 else 'top'\n            axes[i].annotate(f'{height:.3f}', \n                           (p.get_x() + p.get_width() / 2., y_pos),\n                           ha='center', va=va, fontweight='bold', fontsize=11,\n                           bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\nif len(categorical_features) < 6:\n    for j in range(len(categorical_features), 6):\n        fig.delaxes(axes[j])\nplt.tight_layout(pad=3.0, w_pad=2.0, h_pad=3.0)\nplt.subplots_adjust(top=0.92) \nplt.show();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:42.696820Z","iopub.execute_input":"2025-11-29T07:20:42.697032Z","iopub.status.idle":"2025-11-29T07:20:44.405299Z","shell.execute_reply.started":"2025-11-29T07:20:42.697016Z","shell.execute_reply":"2025-11-29T07:20:44.404637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Distributions reveal that `employment status` is the most powerful predictor of `loan repayment`, with retirees demonstrating near-perfect performance, in stark contrast to the high default rates of the `unemployed` and `students`. The credit grade system effectively creates a clear risk hierarchy, validating its use in underwriting. Counterintuitively, `Bachelor's` degree holders show lower repayment rates than those with only a `high school education`, suggesting potential debt burdens. Furthermore, `loan purpose` significantly influences outcomes, with productive investments like `home loans` outperforming consumptive ones like education or medical loans. In contrast, demographic factors like gender and marital status show negligible impact, indicating modern underwriting has successfully moved beyond these characteristics.","metadata":{}},{"cell_type":"markdown","source":"## Categorical Associations Heatmap","metadata":{}},{"cell_type":"code","source":"cramer_matrix = pd.DataFrame(index=categorical_features, columns=categorical_features)\n\nfor i, feat1 in enumerate(categorical_features):\n    for j, feat2 in enumerate(categorical_features):\n        if i <= j:  # Compute only upper triangle to avoid redundant calculations\n            contingency_table = pd.crosstab(df[feat1], df[feat2])\n            chi2 = stats.chi2_contingency(contingency_table, correction=False)[0]\n            n = contingency_table.sum().sum()\n            min_dim = min(contingency_table.shape) - 1\n            cramer_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0\n            \n            cramer_matrix.loc[feat1, feat2] = cramer_v\n            cramer_matrix.loc[feat2, feat1] = cramer_v\n\n# Convert to float type and handle diagonal\ncramer_matrix = cramer_matrix.astype(float)\nnp.fill_diagonal(cramer_matrix.values, 1.0)\n\n# Plot heatmap\nplt.figure(figsize=(14, 10))\nmask = np.triu(np.ones_like(cramer_matrix, dtype=bool))\nsns.heatmap(cramer_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n            mask=mask, center=0.3, vmin=0, vmax=1,\n            cbar_kws={'label': \"Cramer's V (Association Strength)\"},\n            linewidths=0.5, annot_kws={'size': 12})\nplt.title(\"Association Strength Between Categorical Features (Cramer's V)\", \n          fontweight='bold', fontsize=16, pad=20)\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:44.406218Z","iopub.execute_input":"2025-11-29T07:20:44.406709Z","iopub.status.idle":"2025-11-29T07:20:46.522718Z","shell.execute_reply.started":"2025-11-29T07:20:44.406683Z","shell.execute_reply":"2025-11-29T07:20:46.522006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The heatmap reveals extremely weak associations between all categorical features, with Cramer's V values ranging from `0.00` to `0.05`. The diagonal (self-association) is normalized to `1.0`, while all cross-feature associations show negligible relationships:\n* Strongest association: `0.05` between grade_subgrade and employment_status (still considered very weak in statistical terms);\n* Weakest associations: Near-zero values (`0.00` - `0.01`) between gender and all other features;\n* No meaningful clusters: No visible patterns or groupings of related features.\n\n\nSo, credit grades operate on fundamentally different criteria than demographic factors, loan purposes distribute uniformly across all segments enabling demographic-independent marketing strategies, and gender shows no meaningful associations with other features, providing statistical evidence of gender-neutral underwriting. This feature independence creates significant modeling advantages: interaction terms will yield minimal value, dimensionality reduction is unnecessary, and numerical features (credit score, DTI) should be prioritized for predictive power. The only relationship warranting deeper investigation is the weak association (`0.05`) between credit grades and employment status, which could reveal important risk patterns if analyzed through stratified default rates and causal inference. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(36, 12))\n\nunique_marital = df['marital_status'].unique()\nunique_education = df['education_level'].unique()\ntotal_combinations = len(unique_marital) * len(unique_education)\n\nprops = {}\nfor i, marital in enumerate(unique_marital):\n    for j, education in enumerate(unique_education):\n        norm_val = (i * len(unique_education) + j) / total_combinations\n        color = plt.cm.coolwarm(1 - norm_val) \n        props[(marital, education)] = {'color': color}\n\n\nmosaic_data = df[['marital_status', 'education_level']]\nax = mosaic(mosaic_data, ['marital_status', 'education_level'], \n            labelizer=lambda x: '',  \n            properties=props,\n            gap=0.02,\n            ax=plt.gca())\n\n\nplt.title('Proportional Distribution: Marital Status vs Education Level', \n          fontweight='bold', fontsize=16, pad=20)\n\nplt.xlabel('Marital Status', fontsize=14)\nplt.ylabel('Education Level', fontsize=14)\n\n\nfrom matplotlib.patches import Patch\nlegend_elements = []\nfor i, marital in enumerate(unique_marital):\n    for j, education in enumerate(unique_education):\n        norm_val = (i * len(unique_education) + j) / total_combinations\n        color = plt.cm.coolwarm(1 - norm_val)\n        if ((df['marital_status'] == marital) & (df['education_level'] == education)).any():\n            legend_elements.append(Patch(facecolor=color, \n                                       edgecolor='black',\n                                       label=f'{marital}, {education}'))\n\n\nif len(legend_elements) > 0:\n    plt.legend(handles=legend_elements, \n              loc='upper right', \n              bbox_to_anchor=(1.18, 1.0),\n              title='Category Combinations',\n              fontsize=9,\n              framealpha=0.9)\n\nplt.tight_layout(rect=[0, 0.03, 0.85, 1])  \nplt.show();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:46.523616Z","iopub.execute_input":"2025-11-29T07:20:46.523942Z","iopub.status.idle":"2025-11-29T07:20:48.765526Z","shell.execute_reply.started":"2025-11-29T07:20:46.523920Z","shell.execute_reply":"2025-11-29T07:20:48.764812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The mosaic plot demonstrates strong demographic clustering where education level and marital status are more interconnected than previously indicated by the Cramer's V analysis, suggesting that these features may have more nuanced relationships when viewed proportionally. Loan Purpose Distribution by Employment Status indicates that loan purpose is largely independent of employment status (corroborating earlier Cramer's V findings), with debt consolidation being a universal need across all employment segments.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Statistical Tests","metadata":{}},{"cell_type":"markdown","source":"### Welch's t-test with Bonferroni correction for DTI","metadata":{}},{"cell_type":"code","source":"repaid_dti = df[df['loan_paid_back'] == 1]['debt_to_income_ratio']\ndefaulted_dti = df[df['loan_paid_back'] == 0]['debt_to_income_ratio']\nt_stat, p_value_ttest = ttest_ind(repaid_dti, defaulted_dti, equal_var=False)\nprint(f\"Welch's t-test for DTI: t={t_stat:.2f}, p={p_value_ttest:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:48.766356Z","iopub.execute_input":"2025-11-29T07:20:48.766793Z","iopub.status.idle":"2025-11-29T07:20:48.817894Z","shell.execute_reply.started":"2025-11-29T07:20:48.766769Z","shell.execute_reply":"2025-11-29T07:20:48.817251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Welch's t-test with Bonferroni correction for debt-to-income ratio (DTI) between repayment groups revealed extreme statistical significance (t = `-224.67`, p < `0.001`), with non-overlapping interquartile ranges confirming DTI as the strongest univariate predictor of default behavior.","metadata":{}},{"cell_type":"markdown","source":"### Mann-Whitney U test for DTI && Residual analysis for interest rates","metadata":{}},{"cell_type":"code","source":"u_stat, p_value_mwu = mannwhitneyu(repaid_dti, defaulted_dti, alternative='two-sided')\nprint(f\"Mann-Whitney U test for DTI: U={u_stat:.2f}, p={p_value_mwu:.6f}\")\n\n\nX_causal = df[['credit_score', 'debt_to_income_ratio']].copy()\nX_causal = sm.add_constant(X_causal)\ny_causal = df['interest_rate']\nmodel_causal = sm.OLS(y_causal, X_causal).fit()\ndf['interest_rate_residual'] = model_causal.resid\n\nresidual_corr = df['interest_rate_residual'].corr(df['loan_paid_back'])\nprint(f\"Correlation between interest rate residuals and repayment: {residual_corr:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:48.818640Z","iopub.execute_input":"2025-11-29T07:20:48.818842Z","iopub.status.idle":"2025-11-29T07:20:48.967273Z","shell.execute_reply.started":"2025-11-29T07:20:48.818825Z","shell.execute_reply":"2025-11-29T07:20:48.966527Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Mann-Whitney U test for non-normally distributed features (U = `16144766898.00`, p < `0.001`) validated DTI's predictive power under non-parametric assumptions, essential given the pronounced skewness (`1.41`) in it's distribution.","metadata":{}},{"cell_type":"markdown","source":"### Chi-square tests for categorical features","metadata":{}},{"cell_type":"code","source":"def chi_square_test(feature):\n    contingency_table = pd.crosstab(df[feature], df['loan_paid_back'])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    return chi2, p, contingency_table\n\n# Employment status chi-square test\nchi2_emp, p_emp, emp_table = chi_square_test('employment_status')\nprint(f\"Employment Status Chi-square: œá¬≤={chi2_emp:.1f}, p={p_emp:.6f}\")\n\n# Grade_subgrade chi-square test \nchi2_grade, p_grade, grade_table = chi_square_test('grade_subgrade')\nprint(f\"Grade_subgrade Chi-square: œá¬≤={chi2_grade:.1f}, p={p_grade:.6f}\")\n\n# Gender chi-square test\nchi2_gender, p_gender, gender_table = chi_square_test('gender')\nprint(f\"Gender Chi-square: œá¬≤={chi2_gender:.2f}, p={p_gender:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:48.968009Z","iopub.execute_input":"2025-11-29T07:20:48.968239Z","iopub.status.idle":"2025-11-29T07:20:49.182799Z","shell.execute_reply.started":"2025-11-29T07:20:48.968224Z","shell.execute_reply":"2025-11-29T07:20:49.182172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Chi-square tests of independence demonstrated statistically significant associations between categorical features and repayment outcomes, with `employment status` (œá¬≤ = `256,259.9`, p < `0.001`) and `grade_subgrade` (œá¬≤ = `30,871.2`, p < `0.001`) showing the strongest relationships, while `gender` exhibited minimal association (œá¬≤ = `32.81`, p = `0.0000`) after controlling for financial variables.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(24, 12))\nplt.subplot(1, 2, 1)\n\n\ndf['dti_binned'] = pd.cut(df['debt_to_income_ratio'], bins=20)\ndti_default_rate = df.groupby('dti_binned')['loan_paid_back'].apply(lambda x: (x == 0).mean()).reset_index()\ndti_default_rate.columns = ['dti_bin', 'default_rate']\n\n\nplt.plot([interval.mid for interval in dti_default_rate['dti_bin']], \n         dti_default_rate['default_rate'], 'b-', linewidth=2.5)\nplt.axvline(x=0.15, color='r', linestyle='--', alpha=0.7, label='Threshold at 0.15')\nplt.title('Non-linear Relationship: DTI vs Default Probability', fontsize=14)\nplt.xlabel('Debt-to-Income Ratio (midpoint of bins)', fontsize=12)\nplt.ylabel('Default Rate', fontsize=12)\nplt.grid(alpha=0.3)\nplt.legend(loc='lower right')\n\n\nplt.subplot(1, 2, 2)\nX = df[['debt_to_income_ratio']].values\ny = df['loan_paid_back'].values\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred_prob = model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nroc_auc = roc_auc_score(y_test, y_pred_prob)\nplt.plot(fpr, tpr, 'o-', color='darkorange', linewidth=2.5, label=f'ROC curve (area = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1.5)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curve for DTI as Single Predictor', fontsize=14)\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:49.183481Z","iopub.execute_input":"2025-11-29T07:20:49.183698Z","iopub.status.idle":"2025-11-29T07:20:50.168143Z","shell.execute_reply.started":"2025-11-29T07:20:49.183682Z","shell.execute_reply":"2025-11-29T07:20:50.167479Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations**:\n\n1.  **DTI VS Default Probability**: This plot shows how default rates change as DTI increases, with a clear threshold effect at `0.15` (marked with a red dashed line). The plot demonstrates that default probability increases **non-linearly** with DTI, with a particularly sharp increase after the `0.15` threshold.\n2.  **ROC Curve for DTI as Single Predictor**: This plot shows the ROC curve when using only DTI to predict loan repayment. The orange line represents the actual model performance (with *AUC* = `0.717` as shown in the legend), while the blue dashed line represents the baseline random classifier. The plot demonstrates that DTI alone has meaningful predictive power.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"The statistical evidence supports a multi-stage modeling approach with the following components:\n\n* Implement grade_subgrade as an ordinal feature with polynomial transformations to capture non-linear risk progression, particularly at the C-to-D transition where risk increases disproportionately;\n* It should be developed specialized models for high-risk segments (unemployed, DTI > `0.2`) and low-risk segments (`retired`), recognizing the extreme heterogeneity in default rates across employment status categories;\n* It's required to exclude interest rate residuals from the model (r = -0.0069) to prevent penalizing appropriately priced high-risk loans while maintaining the risk-based pricing information through credit score and DTI.\n\n\nThis analysis confirms that credit risk assessment should prioritize the financial capacity dimension (DTI and its non-linear transformations) and creditworthiness dimension (credit score and grade_subgrade with appropriate encoding) as the primary risk determinants. The critical threshold at DTI = `0.15` represents a fundamental risk boundary that should inform underwriting policies and model development.\n\nThe statistical evidence demonstrates that interest rates function primarily as a risk-based pricing mechanism rather than a causal factor in default, a finding with significant implications for regulatory compliance and model fairness. By focusing on the underlying risk factors (credit score and DTI) rather than their pricing manifestation, the model achieves both superior predictive performance and regulatory robustness.\n\nThe optimal modeling strategy integrates these insights through a hybrid approach that preserves the ordinal structure of credit grades while accommodating non-linear risk jumps, and develops specialized submodels for critical risk segments. This approach maximizes the AUC while ensuring business-relevant performance metrics, providing a foundation for both predictive accuracy and actionable risk insights.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"fe\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Feature Engineering ‚öôÔ∏è | üõ†Ô∏è</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"While one-hot encoding was initially implemented for categorical variables, ordinal encoding was strategically selected for credit grades to preserve the intrinsic hierarchical structure of credit risk stratification, wherein `A` represents optimal creditworthiness and `F` denotes the highest risk tier. This methodological choice proves particularly advantageous for gradient-boosting frameworks like CatBoost, which can effectively identify optimal decision boundaries along the ordinal continuum, thereby enhancing model stability, accelerating convergence rates, and improving generalization capacity. Furthermore, ordinal encoding circumvents the dimensional explosion inherent in one-hot representations.\n\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"Creating a copy of the data before the transformations & Removing the target variable and id from the training data","metadata":{}},{"cell_type":"code","source":"X_train = df.drop(['id', 'loan_paid_back'], axis=1)\ny_train = df['loan_paid_back']\nX_test = test.drop('id', axis=1)\nX_test_ids = test['id'].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:50.171793Z","iopub.execute_input":"2025-11-29T07:20:50.172156Z","iopub.status.idle":"2025-11-29T07:20:50.227006Z","shell.execute_reply.started":"2025-11-29T07:20:50.172140Z","shell.execute_reply":"2025-11-29T07:20:50.226440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`annual_income` and `loan_amount` have right‚Äëskew >‚ÄØ`1.5` (see earlier EDA). Log‚Äëtype transforms compress the long tail, making the relationship with the target more linear for gradient‚Äëboosting models. **`Yeo‚ÄëJohnson`** is chosen over a simple `np.log1p` because it also handles possible **zero/negative** values gracefully.\n\n\nEven after a log‚Äëlike transform, a handful of ultra‚Äëlarge incomes/loans can dominate tree splits. **Winsorizing** stabilises the model without discarding data.\n\nAdding ratio & interaction features:\n\n1. Loan‚Äëto‚ÄëIncome is a classic underwriting metric ‚Äì borrowers with a high loan relative to income are riskier;\n2. Interest‚Äëto‚ÄëCredit‚ÄëScore captures the pricing decision; a high interest on a low credit score may amplify default risk;\n3. Income‚Äëto‚ÄëDTI flips the DTI to a more monotonic scale (larger ‚Üí safer).\n\nThe correlation matrix showed a moderate linear relationship, but we know from the EDA that the effect is non‚Äëlinear (e.g., a sharp rise in default after DTI‚ÄØ =‚ÄØ`0.15`). Explicit polynomial terms give the model a head‚Äëstart to capture curvature. Also adding squared terms which can help in cases when regularisation is strong.\n\nWe also expose the ordinal nature explicitly of `grade_subgrade` (instead of one‚Äëhotting 30 categories, which would be wasteful).\n\nBinary flags for high‚Äërisk groups (`Unemployed`, `Student`) improve interpretability and reduce noise from low‚Äëfrequency categories.","metadata":{}},{"cell_type":"code","source":"# identify strongly right‚Äëskewed numeric columns\nnumeric_cols = ['annual_income', 'loan_amount', 'debt_to_income_ratio',\n                'interest_rate', 'credit_score']\nskewness = X_train[numeric_cols].skew()\nskewed_features = skewness[skewness.abs() > 0.75].index.tolist()\n\n\n# log (Yeo‚ÄëJohnson) transformations for skewed numerics\npt = PowerTransformer(method='yeo-johnson', standardize=False)\nX_train[skewed_features] = pt.fit_transform(X_train[skewed_features])\nX_test[skewed_features] = pt.transform(X_test[skewed_features])\n\n\n# winsorize at the 0.5‚ÄØ% / 99.5‚ÄØ% percentiles \ndef winsorize_series(s, lower=0.005, upper=0.995):\n    q_low, q_high = s.quantile([lower, upper])\n    return s.clip(lower=q_low, upper=q_high)\nwinsor_cols = ['annual_income', 'loan_amount']\nfor col in winsor_cols:\n    X_train[col] = winsorize_series(X_train[col])\n    X_test[col] = winsorize_series(X_test[col])\n\n\n# loan_to_income_ratio\nX_train['loan_to_income_ratio'] = X_train['loan_amount'] / X_train['annual_income']\nX_test['loan_to_income_ratio'] = X_test['loan_amount'] / X_test['annual_income']\n\n\n# payment_to_income\nX_train['payment_to_income'] = X_train['loan_amount'] * (1 + X_train['interest_rate']/100) / X_train['annual_income']\nX_test['payment_to_income'] = X_test['loan_amount'] * (1 + X_test['interest_rate']/100) / X_test['annual_income']\n\n\n# interest_burden\nX_train['interest_burden'] = X_train['interest_rate'] * X_train['loan_to_income_ratio']\nX_test['interest_burden'] = X_test['interest_rate'] * X_test['loan_to_income_ratio']\n\n\n# high dti\nX_train['high_dti'] = (X_train['debt_to_income_ratio'] > 0.15).astype(int)\nX_test['high_dti'] = (X_test['debt_to_income_ratio'] > 0.15).astype(int)\n\n\n# extra high dti\nX_train['very_high_dti'] = (X_train['debt_to_income_ratio'] > 0.25).astype(int)\nX_test['very_high_dti'] = (X_test['debt_to_income_ratio'] > 0.25).astype(int)\n\n\n# low_interest_high_loan\nX_train['low_interest_high_loan'] = ((X_train['interest_rate'] < 7) & (X_train['loan_amount'] > 15000)).astype(int)\nX_test['low_interest_high_loan'] = ((X_test['interest_rate'] < 7) & (X_test['loan_amount'] > 15000)).astype(int)\n\n\nif 'loan_grade' in X_train.columns:\n    grade_map_ord = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n    X_train['loan_grade_ord'] = X_train['loan_grade'].map(grade_map_ord)\n    X_test['loan_grade_ord'] = X_test['loan_grade'].map(grade_map_ord)\n\n\n# polynomial features\npoly_cols = ['debt_to_income_ratio', 'interest_rate', 'credit_score']\npoly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\npoly_features = poly.fit_transform(X_train[poly_cols])\npoly_features_test = poly.transform(X_test[poly_cols])\npoly_names = [f\"poly_{i}\" for i in range(poly_features.shape[1])]\nX_train = pd.concat([X_train.reset_index(drop=True), pd.DataFrame(poly_features, columns=poly_names)], axis=1)\nX_test = pd.concat([X_test.reset_index(drop=True), pd.DataFrame(poly_features_test, columns=poly_names)], axis=1)\n\n\n# interest_to_credit_ratio (captures pricing VS risk)\nX_train['interest_credit_ratio'] = X_train['interest_rate'] / X_train['credit_score']\nX_test['interest_credit_ratio'] = X_test['interest_rate'] / X_test['credit_score']\n\n\n# income_to_dti (inverse of dti, easier to interpret)\nX_train['income_to_dti'] = X_train['annual_income'] / X_train['debt_to_income_ratio']\nX_test['income_to_dti'] = X_test['annual_income'] / X_test['debt_to_income_ratio']\n\n\n# debt Service Ratio (DSR): close to debt_to_income_ratio but converts annual financial metrics to monthly equivalents\nX_train['dsr'] = (X_train['loan_amount'] * (X_train['interest_rate'] / 100) / 12) / (X_train['annual_income'] / 12)\nX_test['dsr'] = (X_test['loan_amount'] * (X_test['interest_rate'] / 100) / 12) / (X_test['annual_income'] / 12)\n\n\n# monthly income\nX_train['monthly_income'] = X_train['annual_income'] / 12\nX_test['monthly_income'] = X_test['annual_income'] / 12\n\n\n# credit utilization \nX_train['credit_utilization_proxy'] = (\n    X_train['debt_to_income_ratio'] * \n    (1 - X_train['credit_score'] / 850) * \n    np.log1p(X_train['loan_amount'])\n)\n\nX_test['credit_utilization_proxy'] = (\n    X_test['debt_to_income_ratio'] * \n    (1 - X_test['credit_score'] / 850) * \n    np.log1p(X_test['loan_amount'])\n)\n\n# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º\nscaler_cu = StandardScaler()\nX_train['credit_utilization_proxy'] = scaler_cu.fit_transform(X_train[['credit_utilization_proxy']])\nX_test['credit_utilization_proxy'] = scaler_cu.transform(X_test[['credit_utilization_proxy']])\n\npoly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\ndti_credit_cols = ['debt_to_income_ratio', 'credit_score']\ndti_credit = X_train[dti_credit_cols].values\ndti_credit_test = X_test[dti_credit_cols].values\npoly_features = poly.fit_transform(dti_credit)\npoly_features_test = poly.transform(dti_credit_test)\nfeature_names = poly.get_feature_names_out(dti_credit_cols)\nfor i, name in enumerate(feature_names):\n    if name not in dti_credit_cols:\n        clean_name = f'poly_{name.replace(\" \", \"_\")}'\n        X_train[clean_name] = poly_features[:, i]\n        X_test[clean_name] = poly_features_test[:, i]\n\n\n# squared terms\nX_train['dti_sq'] = X_train['debt_to_income_ratio'] ** 2\nX_test['dti_sq'] = X_test['debt_to_income_ratio'] ** 2\n\nX_train['credit_sq'] = X_train['credit_score'] ** 2\nX_test['credit_sq'] = X_test['credit_score'] ** 2\n\n\n# dti bins\nX_train['dti_binned'] = pd.cut(\n    X_train['debt_to_income_ratio'],\n    bins=[0, 0.1, 0.15, 0.2, 1.0],\n    labels=['low_risk', 'medium_low_risk', 'medium_high_risk', 'high_risk'],\n    include_lowest=True\n)\nX_test['dti_binned'] = pd.cut(\n    X_test['debt_to_income_ratio'],\n    bins=[0, 0.1, 0.15, 0.2, 1.0],\n    labels=['low_risk', 'medium_low_risk', 'medium_high_risk', 'high_risk'],\n    include_lowest=True\n)\n# the sub‚Äëgrade is inherently ordered (A1-F5); \n# map to 1‚Äë30\ngrade_map = {g: i for i, g in enumerate(['A1','A2','A3','A4','A5',\n                                         'B1','B2','B3','B4','B5',\n                                         'C1','C2','C3','C4','C5',\n                                         'D1','D2','D3','D4','D5',\n                                         'E1','E2','E3','E4','E5',\n                                         'F1','F2','F3','F4','F5'], start=1)}\nX_train['grade_ordinal'] = X_train['grade_subgrade'].map(grade_map)\nX_test['grade_ordinal'] = X_test['grade_subgrade'].map(grade_map)\n\n\n# employment status\nfor cat in ['Retired', 'Unemployed', 'Student']:\n    col = f\"is_{cat.lower()}\"\n    X_train[col] = (X_train['employment_status'] == cat).astype(int)\n    X_test[col] = (X_test['employment_status'] == cat).astype(int)\n\n\n# loan purpose: especially ‚Äúdebt consolidation‚Äù dominates\nX_train['is_debt_consolidation'] = (X_train['loan_purpose'] == 'Debt consolidation').astype(int)\nX_test['is_debt_consolidation'] = (X_test['loan_purpose'] == 'Debt consolidation').astype(int)\n\n\n# drop transformed features\nfeatures_to_drop = ['grade_subgrade', 'employment_status', 'loan_purpose']\nfor feature in features_to_drop:\n    if feature in X_train.columns:\n        X_train.drop(feature, axis=1, inplace=True)\n    if feature in X_test.columns:\n        X_test.drop(feature, axis=1, inplace=True)\n\n\n# ensuring consistent column order\nall_features = sorted(list(set(X_train.columns.tolist() + X_test.columns.tolist())))\nX_train = X_train.reindex(columns=all_features, fill_value=0)\nX_test = X_test.reindex(columns=all_features, fill_value=0)\n\n\n# categorical features\ncategorical_features = ['gender',\n                        'marital_status',\n                        'education_level',\n                        'dti_binned']\n\n\n# numerical features\nnumeric_features = X_train.select_dtypes(include=['int64','float64']).columns.tolist()\nnumeric_features = [c for c in numeric_features if c not in categorical_features]\n\n\n# scale numerical features\nscaler = StandardScaler()\nX_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\nX_test[numeric_features] = scaler.transform(X_test[numeric_features])\nX_train[categorical_features] = X_train[categorical_features].astype('category')\nX_test[categorical_features] = X_test[categorical_features].astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:50.227759Z","iopub.execute_input":"2025-11-29T07:20:50.228019Z","iopub.status.idle":"2025-11-29T07:20:52.688802Z","shell.execute_reply.started":"2025-11-29T07:20:50.227991Z","shell.execute_reply":"2025-11-29T07:20:52.688164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:52.689483Z","iopub.execute_input":"2025-11-29T07:20:52.689709Z","iopub.status.idle":"2025-11-29T07:20:52.711697Z","shell.execute_reply.started":"2025-11-29T07:20:52.689692Z","shell.execute_reply":"2025-11-29T07:20:52.711016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# validation split\nX_train_final, X_val, y_train_final, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=SEED, stratify=y_train\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:20:52.712375Z","iopub.execute_input":"2025-11-29T07:20:52.712553Z","iopub.status.idle":"2025-11-29T07:20:53.116319Z","shell.execute_reply.started":"2025-11-29T07:20:52.712539Z","shell.execute_reply":"2025-11-29T07:20:53.115463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"modeling\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Modeling ü§ñ</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"The selection of **CatBoost** as our primary modeling architecture was driven by three critical considerations: \n* It's intrinsic capacity to process mixed data types without requiring one-hot encoding transformations, thereby preserving ordinal relationships and reducing feature space dimensionality;\n* It's advanced handling of imbalanced classification scenarios through configurable class weighting mechanisms;\n* It's computational efficiency when leveraging GPU acceleration for large-scale optimization tasks. Unlike conventional approaches that separate feature engineering from model training, CatBoost integrates these processes through its specialized categorical feature processing pipeline, which employs Bayesian target statistics with random permutations to minimize overfitting while maintaining high predictive fidelity.\n\nTo ensure model robustness and generalizability beyond single-split validation paradigms, we implemented a **stratified 7-fold cross-validation strategy**. This approach systematically partitions the dataset while preserving the class distribution in each fold, providing a statistically rigorous estimation of model performance across diverse data subsets. The stratification protocol is particularly crucial in our context given the pronounced class imbalance (approximately 4:1 ratio), ensuring that each fold contains representative samples from both positive and negative classes. The cross-validation framework yields out-of-fold (OOF) predictions that serve dual purposes: providing an unbiased performance estimation through area under the receiver operating characteristic curve (AUC-ROC), and generating ensemble predictions through averaging across fold-specific models.\n\nHyperparameter optimization represents a pivotal component of our methodology, where we employed Bayesian optimization via the Optuna framework to navigate CatBoost's extensive hyperparameter space efficiently.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h2 id = \"optuna\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Optuna üî¨</b>\n    </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# uncomment for hyperparameter tuning\n#def objective_catboost(trial):\n#    params = {\n#        'iterations': trial.suggest_int('iterations', 1600, 1690),\n#        'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.3, log=True),\n#        'depth': trial.suggest_int('depth', 4, 5),\n#        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 30, 60, log=True),\n#        'border_count': trial.suggest_int('border_count', 230, 240),\n#        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.004, 0.07),\n#        'random_strength': trial.suggest_float('random_strength', 0.48, 0.78),\n#        'scale_pos_weight': 4.0,\n#        'eval_metric': 'F1',\n#        'loss_function': 'Logloss',\n#        'task_type': 'GPU',\n#        'devices': '0-1',\n#        'random_state': SEED,\n#        'verbose': False,\n#        'early_stopping_rounds': 50,\n#        'cat_features': categorical_features\n#    }\n    \n#    model = CatBoostClassifier(**params)\n#    model.fit(\n#        X_train_final, y_train_final,\n#        eval_set=(X_val, y_val),\n#        use_best_model=True,\n#        verbose=False\n#    )\n    \n#    y_pred_proba = model.predict_proba(X_val)[:, 1]\n#    auc_score = roc_auc_score(y_val, y_pred_proba)\n#    return auc_score\n\n#study_catboost = optuna.create_study(direction='maximize', study_name='catboost_optimization')\n#study_catboost.optimize(objective_catboost, n_trials=500)\n\n\n#print(f\"Best AUC score: {study_catboost.best_value:.5f}\")\n#print(\"\\nBest hyperparameters:\")\n#for param_name, param_value in study_catboost.best_params.items():\n#    print(f\"  {param_name}: {param_value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:38:06.255803Z","iopub.execute_input":"2025-11-29T07:38:06.256300Z","iopub.status.idle":"2025-11-29T07:38:06.259847Z","shell.execute_reply.started":"2025-11-29T07:38:06.256278Z","shell.execute_reply":"2025-11-29T07:38:06.259195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h2 id = \"catboost\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>CatBoost üêà | üöÄ</b>\n    </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# best params\n#catboost_best_params = study_catboost.best_params\n#catboost_best_params.update({\n#    'eval_metric': 'F1',\n#    'loss_function': 'Logloss',\n#    'task_type': 'GPU',\n#    'devices': '0-1',\n#    'random_state': SEED,\n#    'verbose': False,\n#    'early_stopping_rounds': 50,\n#    'scale_pos_weight': 4.0,\n#    'cat_features': categorical_features\n#})\n\ncatboost_best_params = {\n    'iterations': 1603,          \n    'learning_rate': 0.2560851269636987,      \n    'depth': 4,                  \n    'l2_leaf_reg': 54.96505861247231,\n    'border_count': 240, \n    'bagging_temperature': 0.036850876921115924,\n    'random_strength': 0.6490047103570082,\n    'scale_pos_weight': 4.0,\n    'eval_metric': 'F1',\n    'loss_function': 'Logloss',\n    'task_type': 'GPU',\n    'devices': '0-1',\n    'random_state': SEED,\n    'verbose': 100,\n    'early_stopping_rounds': 50,\n    'cat_features': categorical_features\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:39:22.716304Z","iopub.execute_input":"2025-11-29T07:39:22.716975Z","iopub.status.idle":"2025-11-29T07:39:22.721157Z","shell.execute_reply.started":"2025-11-29T07:39:22.716954Z","shell.execute_reply":"2025-11-29T07:39:22.720431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"xgb\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>XgBoost ü§ñ</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# xgb params\nxgb_best_params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 5,\n    'n_estimators': 2000,\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'enable_categorical': True,\n    'random_state': SEED\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:41:50.967036Z","iopub.execute_input":"2025-11-29T07:41:50.967827Z","iopub.status.idle":"2025-11-29T07:41:50.971892Z","shell.execute_reply.started":"2025-11-29T07:41:50.967792Z","shell.execute_reply":"2025-11-29T07:41:50.971086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"lgb\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>LightGBM üëæ</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# lgb params\nlgb_best_params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'max_depth': -1,\n    'min_data_in_leaf': 50,\n    'lambda_l1': 5,\n    'lambda_l2': 5,\n    'categorical_feature': 'auto',\n    'boosting_type': 'gbdt',\n    'verbosity': -1,\n    'device': 'gpu',\n    'random_state': SEED\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:41:52.558883Z","iopub.execute_input":"2025-11-29T07:41:52.559466Z","iopub.status.idle":"2025-11-29T07:41:52.563494Z","shell.execute_reply.started":"2025-11-29T07:41:52.559444Z","shell.execute_reply":"2025-11-29T07:41:52.562776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"stack\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Stacking üß±|üß†</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## K-fold cross-validation","metadata":{}},{"cell_type":"code","source":"k = 9\nskf = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n\n\ncatboost_oof_preds = np.zeros(len(X_train))\nxgb_oof_preds = np.zeros(len(X_train))\nlgb_oof_preds = np.zeros(len(X_train))\n\n\ncatboost_test_preds = np.zeros(len(X_test))\nxgb_test_preds = np.zeros(len(X_test))\nlgb_test_preds = np.zeros(len(X_test))\n\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"\\n Fold {fold+1}/{k}\")\n    fold_start_time = time.time()\n    X_train_fold = X_train.iloc[train_idx]\n    y_train_fold = y_train.iloc[train_idx]\n    X_valid_fold = X_train.iloc[valid_idx]\n    y_valid_fold = y_train.iloc[valid_idx]\n    \n    # catboost \n    cb_model = CatBoostClassifier(**catboost_best_params)\n    cb_model.fit(\n        X_train_fold, y_train_fold,\n        eval_set=(X_valid_fold, y_valid_fold),\n        use_best_model=True,\n        verbose=False\n    )\n    catboost_oof_preds[valid_idx] = cb_model.predict_proba(X_valid_fold)[:, 1]\n    catboost_test_preds += cb_model.predict_proba(X_test)[:, 1] / k\n    \n    # xgb \n    xgb_model = xgb.XGBClassifier(**xgb_best_params)\n    xgb_model.fit(\n        X_train_fold, y_train_fold,\n        eval_set=[(X_valid_fold, y_valid_fold)],\n        verbose=False\n    )\n    xgb_oof_preds[valid_idx] = xgb_model.predict_proba(X_valid_fold)[:, 1]\n    xgb_test_preds += xgb_model.predict_proba(X_test)[:, 1] / k\n    \n    # lgb\n    lgb_model = lgb.LGBMClassifier(**lgb_best_params)\n    lgb_model.fit(\n        X_train_fold, y_train_fold,\n        eval_set=[(X_valid_fold, y_valid_fold)]\n    )\n    lgb_oof_preds[valid_idx] = lgb_model.predict_proba(X_valid_fold)[:, 1]\n    lgb_test_preds += lgb_model.predict_proba(X_test)[:, 1] / k\n    \n    fold_time = time.time() - fold_start_time\n    print(f\" Fold {fold+1} completed in {fold_time:.2f} seconds\")\n\n# calculating metrics for basic models\ncatboost_auc = roc_auc_score(y_train, catboost_oof_preds)\nxgb_auc = roc_auc_score(y_train, xgb_oof_preds)\nlgb_auc = roc_auc_score(y_train, lgb_oof_preds)\n\nprint(f\"CatBoost OOF AUC: {catboost_auc:.5f}\")\nprint(f\"XGBoost OOF AUC: {xgb_auc:.5f}\")\nprint(f\"LightGBM OOF AUC: {lgb_auc:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:45:26.129537Z","iopub.execute_input":"2025-11-29T07:45:26.130027Z","iopub.status.idle":"2025-11-29T07:50:56.593870Z","shell.execute_reply.started":"2025-11-29T07:45:26.130005Z","shell.execute_reply":"2025-11-29T07:50:56.593127Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Stacking","metadata":{}},{"cell_type":"code","source":"stack_train = np.column_stack((catboost_oof_preds, xgb_oof_preds, lgb_oof_preds))\nstack_test = np.column_stack((catboost_test_preds, xgb_test_preds, lgb_test_preds))\n\n# LogisticRegression\nmeta_model = LogisticRegression(penalty='l2', C=0.1, max_iter=5000, solver='lbfgs', random_state=SEED)\nmeta_model.fit(stack_train, y_train)\nfinal_test_preds = meta_model.predict_proba(stack_test)[:, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:50:56.594903Z","iopub.execute_input":"2025-11-29T07:50:56.595264Z","iopub.status.idle":"2025-11-29T07:50:57.608227Z","shell.execute_reply.started":"2025-11-29T07:50:56.595247Z","shell.execute_reply":"2025-11-29T07:50:57.607624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"submission\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Submission ‚úîÔ∏è</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame({\n    'id': X_test_ids,\n    'loan_paid_back': final_test_preds\n})\n\nsubmission_filename = f'submission.csv'\nsubmission_df.to_csv(submission_filename, index=False)\nprint(f\"\\nSubmission saved to: {submission_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:50:57.608722Z","iopub.execute_input":"2025-11-29T07:50:57.608903Z","iopub.status.idle":"2025-11-29T07:50:58.130983Z","shell.execute_reply.started":"2025-11-29T07:50:57.608888Z","shell.execute_reply":"2025-11-29T07:50:58.130225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:50:58.132465Z","iopub.execute_input":"2025-11-29T07:50:58.132753Z","iopub.status.idle":"2025-11-29T07:50:58.139984Z","shell.execute_reply.started":"2025-11-29T07:50:58.132736Z","shell.execute_reply":"2025-11-29T07:50:58.139250Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SHAP","metadata":{}},{"cell_type":"code","source":"explainer = shap.TreeExplainer(cb_model)\nshap_values = explainer.shap_values(X_val)\n\nplt.figure(figsize=(14, 10))\nshap.summary_plot(shap_values, X_val, max_display = 20, show = False)\nplt.title('SHAP Values - CatBoost Model', fontsize=16, fontweight='bold')\nplt.tight_layout();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:50:58.140744Z","iopub.execute_input":"2025-11-29T07:50:58.141036Z","iopub.status.idle":"2025-11-29T07:51:25.275904Z","shell.execute_reply.started":"2025-11-29T07:50:58.141007Z","shell.execute_reply":"2025-11-29T07:51:25.275178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Meta-Model Feature Importance","metadata":{}},{"cell_type":"code","source":"meta_importance = pd.DataFrame({\n    'Model': ['CatBoost', 'XGBoost', 'LightGBM'],\n    'Importance': np.abs(meta_model.coef_[0])\n}).sort_values('Importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Model', data=meta_importance, palette='viridis')\nplt.title('Meta-Model Feature Importance (Base Model Weights)', fontsize=14, fontweight='bold')\nplt.xlabel('Absolute Coefficient Value', fontsize=12)\nplt.ylabel('Base Model', fontsize=12)\nplt.tight_layout();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:51:25.276723Z","iopub.execute_input":"2025-11-29T07:51:25.277008Z","iopub.status.idle":"2025-11-29T07:51:25.446997Z","shell.execute_reply.started":"2025-11-29T07:51:25.276986Z","shell.execute_reply":"2025-11-29T07:51:25.446268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"summary\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>Summary üí≠</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"The primary contributions of this work are threefold: First, we identify and validate the existence of two orthogonal risk dimensions‚Äîfinancial capacity and creditworthiness‚Äîthat operate largely independently yet jointly determine repayment outcomes. Second, we develop a feature engineering strategy that captures non-linear threshold effects, particularly the critical debt-to-income ratio boundary at `0.15`, which conventional approaches typically overlook. Third, we implement a modeling framework that integrates causal analysis with ensemble learning to distinguish between risk factors and their pricing manifestations. \n\nOur analysis of the dataset revealed profound insights into the structure of credit risk. The distribution of debt-to-income ratio (DTI) displayed pronounced positive skewness (`1.41`), with most borrowers clustered in the conservative range of `0.0` - `0.2`. Crucially, we identified a non-linear threshold effect at DTI = `0.15`, beyond which default probability increases dramatically. This finding challenges conventional linear approaches to risk modeling and suggests the existence of distinct borrower segments separated by critical financial capacity boundaries. In contrast to DTI, credit scores followed a near-perfect normal distribution (skewness = `-0.17`), with a symmetric spread around the `650` - `700` range. This distributional property makes credit scores ideal for parametric modeling while still requiring non-linear transformations to capture their relationship with default probability. The categorical analysis revealed striking patterns in employment status, with retirees demonstrating near-perfect repayment rates (`99.7%`) while unemployed borrowers defaulted at alarming rates (`92.2%`). This `91.9` percentage point difference represents one of the most significant risk differentials observed in credit modeling literature and suggests employment dynamics are substantially underweighted in conventional risk frameworks. Our correlation analysis established two distinct risk clusters. The financial capacity cluster, centered around DTI (`-0.34` correlation with repayment), captures borrowers' ability to service debt obligations. The creditworthiness cluster, represented by credit score (`0.23` correlation), reflects historical repayment behavior. These clusters operate largely independently (Cramer's V < `0.05`), creating modeling opportunities through carefully constructed interaction terms. The grade_subgrade analysis revealed a systematic risk progression across credit tiers (A=`4.8%`, B=`6.8%`, C=`15.3`%, D=`28.5%` default rates), validating the theoretical foundation of credit grading systems while highlighting non-linear risk jumps at critical grade boundaries, particularly the C-to-D transition. For the grade_subgrade feature, we resolved the encoding dilemma through a hybrid approach that preserved ordinal information while accommodating non-linearities. We mapped each subgrade to an ordinal scale based on default rate progression, then supplemented this with polynomial transformations to capture disproportionate risk jumps at grade boundaries. This strategy maintained the interpretability of the grading system while capturing its complex risk properties.\n\n\n\n> üëÄ It might be useful to isolate the age based on `education_level` and `employment_status`, but this seems to be a chaotic solution, since the data is synthetic.\n\n\nGiven the competition's ROC AUC evaluation metric and the dataset's characteristics, we implemented a sophisticated modeling framework centered on CatBoost with GPU-accelerated Optuna optimization. This approach leveraged several key advantages of gradient boosting for credit risk assessment: native handling of mixed data types, robustness to outliers, and automatic feature interaction detection.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color: white;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    border-radius: 5px;\n    background-color: #003247;\n    font-size: 220%;\n    font-family: Nexa;\n    letter-spacing: 0.5px;\n    min-height: 100px;\n    padding: 20px; \n    width: 100%;\n    box-sizing: border-box;\">\n    <h1 id = \"refs\"\n        style=\" color: white;\n        margin: 0;\n        text-align: center;\n        width: 100%;\">\n        <b>References üìú</b>\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"1. [ROC AUC curve explained |Do it with your own hands](https://www.kaggle.com/code/meowmeowmeowmeowmeow/roc-auc-curve-explained-do-it-with-your-own-hands/notebook)\n2. [CatBoost-LightGBM-XGBoost Explained by SHAP](https://www.kaggle.com/code/kaanboke/catboost-lightgbm-xgboost-explained-by-shap)\n3. [Simple XGBoost | Only Competition Data ‚Äì S5E11](https://www.kaggle.com/code/yousefelshahat2/simple-xgboost-only-competition-data-s5e11/notebook)\n4. [Cramer's V correlation matrix](https://www.kaggle.com/code/chrisbss1/cramer-s-v-correlation-matrix)\n5. [Loan Approval | EDA + Catboost + Optuna](https://www.kaggle.com/code/igorvolianiuk/loan-approval-eda-catboost-optuna)\n6. [S4:E10|Loan Status Prediction-CatBoost 97% üéØüìä](https://www.kaggle.com/code/rv1922/s4-e10-loan-status-prediction-catboost-97)\n7. [Loan Prediction Dataset ML Project üìà](https://www.kaggle.com/code/yonatanrabinovich/loan-prediction-dataset-ml-project#The-Process-of-Modeling-the-Data:)","metadata":{}}]}